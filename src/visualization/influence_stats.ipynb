{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e658a417",
   "metadata": {},
   "source": [
    "# Influence Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6794047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, wasserstein_distance\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metrics_tables(\n",
    "    datasets: dict[str, dict[str, Path]],\n",
    "    models: dict[str, tuple[str, str]],\n",
    "    sequence_length: int,\n",
    "    tensor_name: str = \"input_grad_l2_mean.pt\",\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Build one dataframe per dataset with rows=metrics and columns=models.\n",
    "\n",
    "    Args:\n",
    "        datasets: mapping like\n",
    "            {\n",
    "              \"fineweb\": {\"results_root\": Path(...), \"dataset_name\": \"...\", \"rollout_dir\": Path(...)},\n",
    "              ...\n",
    "            }\n",
    "        models: mapping {display_name: (influence_model_dir, rollout_model_dir)}.\n",
    "        sequence_length: sequence length subfolder name.\n",
    "        tensor_name: influence tensor filename.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, pd.DataFrame] with rows [spearman, wasserstein].\n",
    "\n",
    "    \"\"\"\n",
    "    metric_index = [\"spearman\", \"wasserstein\"]\n",
    "    tables: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for dataset_key, dataset_info in datasets.items():\n",
    "        results_root = dataset_info[\"results_root\"]\n",
    "        rollout_root = dataset_info[\"rollout_dir\"]\n",
    "        df = pd.DataFrame(index=metric_index, columns=models.keys(), dtype=float)\n",
    "\n",
    "        for display_name, (influence_model_dir, rollout_model_dir) in models.items():\n",
    "            tensor_path = (\n",
    "                results_root / influence_model_dir / str(sequence_length) / tensor_name\n",
    "            )\n",
    "            influence = torch.load(tensor_path, map_location=\"cpu\")\n",
    "            influence_arr = (\n",
    "                influence.detach().cpu().numpy().astype(np.float64).flatten()\n",
    "            )\n",
    "\n",
    "            rollout_path = (\n",
    "                rollout_root / rollout_model_dir / \"last_row_distribution.csv\"\n",
    "            )\n",
    "            rollout_df = pd.read_csv(rollout_path)\n",
    "            rollout_arr = rollout_df[\"probability\"].to_numpy(dtype=np.float64).flatten()\n",
    "\n",
    "            # Match lengths by trimming to the shorter one\n",
    "            min_len = min(len(influence_arr), len(rollout_arr))\n",
    "            influence_arr = influence_arr[:min_len]\n",
    "            rollout_arr = rollout_arr[:min_len]\n",
    "\n",
    "            # Normalize to probability distributions\n",
    "            influence_arr = influence_arr / influence_arr.sum()\n",
    "            rollout_arr = rollout_arr / rollout_arr.sum()\n",
    "\n",
    "            spearman_corr, _ = spearmanr(influence_arr, rollout_arr)\n",
    "\n",
    "            n = len(influence_arr)\n",
    "            positions = np.arange(n)\n",
    "            wd = wasserstein_distance(\n",
    "                positions,\n",
    "                positions,\n",
    "                u_weights=influence_arr,\n",
    "                v_weights=rollout_arr,\n",
    "            )\n",
    "            wd_norm = wd / (n - 1)\n",
    "\n",
    "            df.loc[\"spearman\", display_name] = round(spearman_corr, 2)\n",
    "            df.loc[\"wasserstein\", display_name] = round(wd_norm, 4)\n",
    "\n",
    "        tables[dataset_key] = df\n",
    "    return tables\n",
    "\n",
    "\n",
    "# Example usage\n",
    "datasets = {\n",
    "    \"fineweb\": {\n",
    "        \"results_root\": Path(\n",
    "            \"/data/shared_data/position-bias/results-final-fineweb-edu\",\n",
    "        ),\n",
    "        \"dataset_name\": \"HuggingFaceFW/fineweb-edu\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/all_content/fineweb-edu\"),\n",
    "    },\n",
    "    \"dclm\": {\n",
    "        \"results_root\": Path(\"/data/shared_data/position-bias/results-final-dclm\"),\n",
    "        \"dataset_name\": \"mlfoundations/dclm-baseline-1.0\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/all_content/dclm\"),\n",
    "    },\n",
    "    \"wikipedia\": {\n",
    "        \"results_root\": Path(\"/data/shared_data/position-bias/results-final-wikipedia\"),\n",
    "        \"dataset_name\": \"wikimedia/wikipedia\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/all_content/wikipedia\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "models = {\n",
    "    # display_name: (influence_model_dir, rollout_model_dir)\n",
    "    \"falcon-rw-7b\": (\"tiiuae_falcon-rw-7b\", \"falcon-rw-7b\"),\n",
    "    \"mpt-7b\": (\"anas-awadalla_mpt-7b\", \"mpt-7b\"),\n",
    "    \"mpt-30b\": (\"eluzhnica_mpt-30b-peft-compatible\", \"mpt-30b\"),\n",
    "    \"bloom-7b1\": (\"bigscience_bloom-7b1\", \"bloom-7b1\"),\n",
    "    \"bloom\": (\"bigscience_bloom\", \"bloom\"),\n",
    "}\n",
    "\n",
    "sequence_length = 256\n",
    "\n",
    "tables_cd = build_metrics_tables(\n",
    "    datasets=datasets,\n",
    "    models=models,\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Figure c and d\n",
    "tables_cd[\"fineweb\"]\n",
    "# tables_cd[\"dclm\"]\n",
    "# tables_cd[\"wikipedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"fineweb\": {\n",
    "        \"results_root\": Path(\n",
    "            \"/data/shared_data/position-bias/results-final-fineweb-edu\",\n",
    "        ),\n",
    "        \"dataset_name\": \"HuggingFaceFW/fineweb-edu\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/no_content\"),\n",
    "    },\n",
    "    \"dclm\": {\n",
    "        \"results_root\": Path(\"/data/shared_data/position-bias/results-final-dclm\"),\n",
    "        \"dataset_name\": \"mlfoundations/dclm-baseline-1.0\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/no_content\"),\n",
    "    },\n",
    "    \"wikipedia\": {\n",
    "        \"results_root\": Path(\"/data/shared_data/position-bias/results-final-wikipedia\"),\n",
    "        \"dataset_name\": \"wikimedia/wikipedia\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/no_content\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "models = {\n",
    "    # display_name: (influence_model_dir, rollout_model_dir)\n",
    "    \"falcon-rw-7b\": (\"tiiuae_falcon-rw-7b\", \"falcon-rw-7b\"),\n",
    "    \"mpt-7b\": (\"anas-awadalla_mpt-7b\", \"mpt-7b\"),\n",
    "    \"mpt-30b\": (\"eluzhnica_mpt-30b-peft-compatible\", \"mpt-30b\"),\n",
    "    \"bloom-7b1\": (\"bigscience_bloom-7b1\", \"bloom-7b1\"),\n",
    "    \"bloom\": (\"bigscience_bloom\", \"bloom\"),\n",
    "}\n",
    "\n",
    "sequence_length = 256\n",
    "\n",
    "tables_bd = build_metrics_tables(\n",
    "    datasets=datasets,\n",
    "    models=models,\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Figure b and d\n",
    "tables_bd[\"fineweb\"]\n",
    "# tables_bd[\"dclm\"]\n",
    "# tables_bd[\"wikipedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"fineweb\": {\n",
    "        \"results_root\": Path(\n",
    "            \"/data/shared_data/position-bias/results-final-fineweb-edu\",\n",
    "        ),\n",
    "        \"dataset_name\": \"HuggingFaceFW/fineweb-edu\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/no_content\"),\n",
    "    },\n",
    "    \"dclm\": {\n",
    "        \"results_root\": Path(\"/data/shared_data/position-bias/results-final-dclm\"),\n",
    "        \"dataset_name\": \"mlfoundations/dclm-baseline-1.0\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/no_content\"),\n",
    "    },\n",
    "    \"wikipedia\": {\n",
    "        \"results_root\": Path(\"/data/shared_data/position-bias/results-final-wikipedia\"),\n",
    "        \"dataset_name\": \"wikimedia/wikipedia\",\n",
    "        \"rollout_dir\": Path(\"../../results/rollout/no_content\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "models = {\n",
    "    # display_name: (influence_model_dir, rollout_model_dir)\n",
    "    \"falcon-rw-7b\": (\"tiiuae_falcon-rw-7b\", \"falcon-rw-7b-noresidual\"),\n",
    "    \"mpt-7b\": (\"anas-awadalla_mpt-7b\", \"mpt-7b-noresidual\"),\n",
    "    \"mpt-30b\": (\"eluzhnica_mpt-30b-peft-compatible\", \"mpt-30b-noresidual\"),\n",
    "    \"bloom-7b1\": (\"bigscience_bloom-7b1\", \"bloom-7b1-noresidual\"),\n",
    "    \"bloom\": (\"bigscience_bloom\", \"bloom-noresidual\"),\n",
    "}\n",
    "\n",
    "sequence_length = 256\n",
    "\n",
    "tables_ad = build_metrics_tables(\n",
    "    datasets=datasets,\n",
    "    models=models,\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Figure a and d\n",
    "tables_ad[\"fineweb\"]\n",
    "# tables_ad[\"dclm\"]\n",
    "# tables_ad[\"wikipedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a combined table for each metric (spearman, wasserstein)\n",
    "for metric in [\"spearman\", \"wasserstein\"]:\n",
    "    rows = []\n",
    "    for model_name in tables_cd[\"fineweb\"].columns:\n",
    "        row = {}\n",
    "        for dataset_key in [\"fineweb\", \"dclm\", \"wikipedia\"]:\n",
    "            row[(dataset_key, \"ad\")] = tables_ad[dataset_key].loc[metric, model_name]\n",
    "            row[(dataset_key, \"bd\")] = tables_bd[dataset_key].loc[metric, model_name]\n",
    "            row[(dataset_key, \"cd\")] = tables_cd[dataset_key].loc[metric, model_name]\n",
    "        rows.append(row)\n",
    "\n",
    "    combined = pd.DataFrame(rows, index=tables_cd[\"fineweb\"].columns)\n",
    "    combined.columns = pd.MultiIndex.from_tuples(\n",
    "        combined.columns,\n",
    "        names=[\"dataset\", \"table\"],\n",
    "    )\n",
    "    combined.index.name = \"model\"\n",
    "\n",
    "    def _highlight_row_by_dataset(s: pd.Series, metric: str) -> pd.Series:\n",
    "        styles = pd.Series(\"\", index=s.index)\n",
    "        for ds in [\"fineweb\", \"dclm\", \"wikipedia\"]:\n",
    "            block = s.loc[ds]  # ad, bd, cd for one dataset in this model row\n",
    "            target = block.max() if metric == \"spearman\" else block.min()\n",
    "            for table_name in block[block == target].index:\n",
    "                styles.loc[(ds, table_name)] = \"font-weight: bold\"\n",
    "        return styles\n",
    "\n",
    "    styled = combined.style.apply(\n",
    "        _highlight_row_by_dataset,\n",
    "        axis=1,\n",
    "        metric=metric,\n",
    "    ).format(\"{:.2f}\")\n",
    "\n",
    "    print(f\"{metric.capitalize()}\")\n",
    "    display(styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9101f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "position-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
