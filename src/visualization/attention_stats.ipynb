{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f129fd",
   "metadata": {},
   "source": [
    "# Attention Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from jaxtyping import Float\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import scienceplots  # noqa: F401\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216de37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/shared_data/position-bias/results-final-wikipedia\"\n",
    "model_name: Literal[\n",
    "    \"anas-awadalla_mpt-7b\",\n",
    "    \"bigscience_bloom\",\n",
    "    \"bigscience_bloom-7b1\",\n",
    "    \"tiiuae_falcon-rw-7b\",\n",
    "    \"eluzhnica_mpt-30b-peft-compatible\",\n",
    "] = \"bigscience_bloom\"\n",
    "\n",
    "sequence_length = 256\n",
    "\n",
    "data_dir = Path(data_path) / model_name / str(sequence_length)\n",
    "mean_attention_matrix: Float[torch.Tensor, \"n_layers n_heads seq_len seq_len\"] = (\n",
    "    torch.load(data_dir / \"qk_mean.pt\")\n",
    ")\n",
    "\n",
    "n_layers, n_heads, seq_len, _ = mean_attention_matrix.shape\n",
    "\n",
    "assert n_layers > 0\n",
    "assert n_heads > 0\n",
    "assert seq_len > 0\n",
    "\n",
    "result_dir = Path(\"../../results/attention\")\n",
    "result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = result_dir / f\"attention_stats_summary_{model_name}_{sequence_length}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals = mean_attention_matrix.reshape(-1)\n",
    "max_samples = 1_000_000\n",
    "if all_vals.numel() > max_samples:\n",
    "    all_vals = all_vals[\n",
    "        torch.randperm(all_vals.numel(), device=all_vals.device)[:max_samples]\n",
    "    ]\n",
    "vmin = torch.quantile(all_vals, 0.005)\n",
    "vmax = torch.quantile(all_vals, 0.995)\n",
    "bins = torch.linspace(vmin, vmax, steps=200, device=all_vals.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_probs(\n",
    "    values: Float[torch.Tensor, \"n_layers n_heads num_values\"],\n",
    "    eps: float = 1e-12,\n",
    ") -> Float[torch.Tensor, \"n_layers n_heads n_bins\"]:\n",
    "    \"\"\"Compute histogram-based probability distributions over specified bins.\"\"\"\n",
    "    n_bins = bins.numel() - 1\n",
    "    idx = torch.bucketize(values, bins, right=False) - 1\n",
    "    valid = (idx >= 0) & (idx < n_bins)\n",
    "\n",
    "    idx = idx.masked_fill(~valid, 0)\n",
    "    counts = torch.zeros(\n",
    "        *values.shape[:-1],\n",
    "        n_bins,\n",
    "        dtype=torch.float64,\n",
    "        device=values.device,\n",
    "    )\n",
    "    ones = torch.ones_like(idx, dtype=counts.dtype)\n",
    "    ones = ones.masked_fill(~valid, 0)\n",
    "    counts.scatter_add_(-1, idx, ones)\n",
    "\n",
    "    probs = counts + eps\n",
    "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def within_similarity(\n",
    "    p: Float[torch.Tensor, \"n_layers n_heads n_bins\"],\n",
    ") -> Float[torch.Tensor, \"n_layers n_heads\"]:\n",
    "    \"\"\"Compute the complement of the normalized Shannon entropy.\"\"\"\n",
    "    ent = -(p * torch.log(p)).sum(dim=-1)\n",
    "    ent_norm = ent / torch.log(\n",
    "        torch.tensor(p.shape[-1], dtype=torch.float64, device=p.device),\n",
    "    )\n",
    "    return 1.0 - ent_norm\n",
    "\n",
    "\n",
    "def print_stats(name: str, values: Float[torch.Tensor, \"n_layers n_heads\"]) -> None:\n",
    "    \"\"\"Print and log basic statistics of the given tensor values.\"\"\"\n",
    "    v = values.detach()\n",
    "    stats = {\n",
    "        \"min\": v.min().item(),\n",
    "        \"max\": v.max().item(),\n",
    "        \"mean\": v.mean().item(),\n",
    "        \"std\": v.std(unbiased=False).item(),\n",
    "    }\n",
    "    line = (\n",
    "        f\"{name} | min={stats['min']:.4f} max={stats['max']:.4f} \"\n",
    "        f\"mean={stats['mean']:.4f} std={stats['std']:.4f}\"\n",
    "    )\n",
    "    print(line)\n",
    "    with log_file.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def js_dissimilarity(\n",
    "    p: Float[torch.Tensor, \"n_layers n_heads n_bins\"],\n",
    "    q: Float[torch.Tensor, \"n_layers n_heads n_bins\"],\n",
    "    eps: float = 1e-12,\n",
    ") -> Float[torch.Tensor, \"n_layers n_heads\"]:\n",
    "    \"\"\"Compute the Jensen-Shannon dissimilarity between two probability distributions.\"\"\"\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    p = p / p.sum(dim=-1, keepdim=True)\n",
    "    q = q / q.sum(dim=-1, keepdim=True)\n",
    "    m = 0.5 * (p + q)\n",
    "    kl_pm = (p * torch.log(p / m)).sum(dim=-1)\n",
    "    kl_qm = (q * torch.log(q / m)).sum(dim=-1)\n",
    "    js = 0.5 * (kl_pm + kl_qm)\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(\n",
    "    data: Float[torch.Tensor, \"n_layers n_heads\"] | np.ndarray,\n",
    "    cbar_label: str,\n",
    "    file_name: str,\n",
    ") -> None:\n",
    "    \"\"\"Plot heatmap of data and save to file.\"\"\"\n",
    "    if torch.is_tensor(data):\n",
    "        data = data.detach().cpu().numpy()\n",
    "    plt.style.use([\"bright\", \"no-latex\"])\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.family\": \"sans-serif\",\n",
    "            \"font.sans-serif\": [\"DejaVu Sans\", \"Liberation Sans\", \"Arial\"],\n",
    "            \"mathtext.fontset\": \"dejavusans\",\n",
    "            \"font.size\": 25,\n",
    "            \"axes.labelsize\": 25,\n",
    "            \"xtick.labelsize\": 20,\n",
    "            \"ytick.labelsize\": 20,\n",
    "        },\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(data, aspect=\"equal\", origin=\"lower\", vmin=0, vmax=1)\n",
    "    ax.set_xlabel(\"Head\")\n",
    "    ax.set_ylabel(\"Layer\")\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax, label=cbar_label)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(result_dir / file_name, dpi=400, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Exported to {result_dir / file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa317fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag: Float[torch.Tensor, \"n_layers n_heads seq_len\"] = mean_attention_matrix.diagonal(\n",
    "    dim1=-2,\n",
    "    dim2=-1,\n",
    ")\n",
    "\n",
    "lower_mask = torch.tril(\n",
    "    torch.ones(seq_len, seq_len, device=mean_attention_matrix.device),\n",
    "    diagonal=-1,\n",
    ").bool()\n",
    "lower: Float[torch.Tensor, \"n_layers n_heads num_lower\"] = mean_attention_matrix[\n",
    "    ...,\n",
    "    lower_mask,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f74030",
   "metadata": {},
   "source": [
    "## Within-diagonal similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e147a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_similarity = within_similarity(hist_probs(diag))\n",
    "print_stats(\"Within-diagonal similarity\", diag_similarity)\n",
    "\n",
    "plot_heatmap(\n",
    "    diag_similarity,\n",
    "    \"Similarity\",\n",
    "    f\"attention_stats_within_diag_{model_name}_{sequence_length}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdab91c",
   "metadata": {},
   "source": [
    "## Within-lower-triangular off diagonal similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d75b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_similarity = within_similarity(hist_probs(lower))\n",
    "print_stats(\"Within-lower similarity\", lower_similarity)\n",
    "\n",
    "plot_heatmap(\n",
    "    lower_similarity,\n",
    "    \"Similarity\",\n",
    "    f\"attention_stats_within_lower_{model_name}_{sequence_length}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b76528",
   "metadata": {},
   "source": [
    "## JS similarity: diagonal vs lower-triangular off-diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95374b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity = js_dissimilarity(hist_probs(diag), hist_probs(lower))\n",
    "js_similarity = 1.0 - dissimilarity\n",
    "print_stats(\"JS similarity\", js_similarity)\n",
    "\n",
    "plot_heatmap(\n",
    "    js_similarity,\n",
    "    \"JS similarity\",\n",
    "    f\"attention_stats_js_similarity_{model_name}_{sequence_length}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026299ab",
   "metadata": {},
   "source": [
    "## Mean content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bf617",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diagonal = diag.mean()\n",
    "mean_off_diagonal = lower.mean()\n",
    "\n",
    "print(f\"Mean diagonal attention: {mean_diagonal:.4f}\")\n",
    "print(f\"Mean off diagonal attention: {mean_off_diagonal:.4f}\")\n",
    "print(f\"Difference: {mean_diagonal - mean_off_diagonal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea2781",
   "metadata": {},
   "source": [
    "## All content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_head_mean_diag = diag.mean(dim=-1)\n",
    "layer_head_mean_lower = lower.mean(dim=-1)\n",
    "\n",
    "print(f\"Diag: {layer_head_mean_diag.tolist()}\")\n",
    "print(f\"Lower: {layer_head_mean_lower.tolist()}\")\n",
    "print(f\"Difference: {(layer_head_mean_diag - layer_head_mean_lower).tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "position-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
