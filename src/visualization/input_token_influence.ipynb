{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1101d976",
   "metadata": {},
   "source": [
    "# Input Token Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45652053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "result_dir = Path(\"../../results/input_token_influence\")\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, StrMethodFormatter\n",
    "import scienceplots  # noqa: F401\n",
    "\n",
    "\n",
    "def style_attn_plot(ax: Axes) -> None:\n",
    "    \"\"\"Apply consistent styling to attention plots.\"\"\"\n",
    "    plt.style.use([\"science\", \"bright\", \"grid\", \"no-latex\"])\n",
    "    # Use sans-serif fonts (incl. mathtext) for cleaner small-scale rendering.\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.family\": \"sans-serif\",\n",
    "            \"font.sans-serif\": [\"DejaVu Sans\", \"Liberation Sans\", \"Arial\"],\n",
    "            \"mathtext.fontset\": \"dejavusans\",\n",
    "        },\n",
    "    )\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    ax.lines[0].set_color(colors[4])\n",
    "    ax.set_xlabel(\"Token position\", fontsize=14)\n",
    "    ax.set_ylabel(\"$\\\\hat{p}^{(T)}(j)$\", fontsize=14)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax.yaxis.set_major_formatter(StrMethodFormatter(\"{x:.3f}\"))\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=11)\n",
    "    ax.margins(x=0.02)\n",
    "    ax.grid(visible=True, which=\"major\", alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_influence(\n",
    "    influence: NDArray,\n",
    ") -> tuple[Figure, Axes]:\n",
    "    \"\"\"Plot input gradient influence scores.\"\"\"\n",
    "    positions = np.arange(len(influence))\n",
    "\n",
    "    assert influence.ndim == 1, (\n",
    "        f\"Expected 1D influence tensor, got shape {influence.shape}\"\n",
    "    )\n",
    "    assert positions.shape[0] == influence.shape[0], (\n",
    "        \"Positions and influence lengths must match\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 3), dpi=400)\n",
    "\n",
    "    ax.plot(positions, influence, marker=\"o\", linewidth=1.5)\n",
    "\n",
    "    style_attn_plot(ax)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def save_figure(fig: Figure, filename: str) -> None:\n",
    "    \"\"\"Save figure to paper figures directory.\"\"\"\n",
    "    fig.savefig(\n",
    "        result_dir / filename,\n",
    "        format=\"pdf\",\n",
    "        dpi=400,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    print(f\"Figure saved to {result_dir / filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\n",
    "    \"/data/shared_data/position-bias/results-final\",\n",
    ")  # Update this path to your saved results folder\n",
    "model: Literal[\n",
    "    \"anas-awadalla_mpt-7b\",\n",
    "    \"bigscience_bloom\",\n",
    "    \"bigscience_bloom-7b1\",\n",
    "    \"tiiuae_falcon-rw-7b\",\n",
    "    \"eluzhnica_mpt-30b-peft-compatible\",\n",
    "] = \"bigscience_bloom\"\n",
    "\n",
    "dataset_name = \"HuggingFaceFW/fineweb-edu\"\n",
    "sequence_length = 256\n",
    "tensor_path = results_dir / model / str(sequence_length) / \"input_grad_l2_mean.pt\"\n",
    "\n",
    "influence = torch.load(tensor_path, map_location=\"cpu\")\n",
    "influence = influence.detach().cpu().numpy().astype(np.float64)\n",
    "influence /= influence.sum()\n",
    "\n",
    "fig, ax = plot_influence(influence)\n",
    "plt.show()\n",
    "\n",
    "save_figure(fig, f\"iti_{model}_{dataset_name.replace('/', '_')}_{sequence_length}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "position-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
