{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f129fd",
   "metadata": {},
   "source": [
    "# Attention Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from jaxtyping import Float\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import plotly.express as px\n",
    "import scienceplots  # noqa: F401\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216de37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/shared_data/position-bias/results-final-fineweb-edu\"\n",
    "model_name: Literal[\n",
    "    \"anas-awadalla_mpt-7b\",\n",
    "    \"bigscience_bloom\",\n",
    "    \"bigscience_bloom-7b1\",\n",
    "    \"tiiuae_falcon-rw-7b\",\n",
    "    \"eluzhnica_mpt-30b-peft-compatible\",\n",
    "] = \"bigscience_bloom\"\n",
    "sequence_length = 64\n",
    "\n",
    "data_dir = Path(data_path) / model_name / str(sequence_length)\n",
    "mean_attention_matrix: Float[torch.Tensor, \"n_layers n_heads seq_len seq_len\"] = (\n",
    "    torch.load(data_dir / \"qk_mean.pt\")\n",
    ")\n",
    "var_attention_matrix: Float[torch.Tensor, \"n_layers n_heads seq_len seq_len\"] = (\n",
    "    torch.load(data_dir / \"qk_var.pt\")\n",
    ")\n",
    "\n",
    "assert mean_attention_matrix.shape == var_attention_matrix.shape\n",
    "\n",
    "n_layers, n_heads, seq_len, _ = mean_attention_matrix.shape\n",
    "\n",
    "result_dir = Path(\"../../results/attention\")\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70033be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(matrix: Float[NDArray, \"seq_len seq_len\"]) -> dict[str, float]:\n",
    "    \"\"\"Compute various statistics of a given matrix.\"\"\"\n",
    "    diag = np.diagonal(matrix)\n",
    "    rows, cols = np.indices(matrix.shape)\n",
    "    dist = np.abs(rows - cols)\n",
    "\n",
    "    abs_matrix = np.abs(matrix)\n",
    "    avg_dist = (\n",
    "        np.sum(dist * abs_matrix) / np.sum(abs_matrix) if np.sum(abs_matrix) > 0 else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Min\": matrix.min(),\n",
    "        \"Max\": matrix.max(),\n",
    "        \"Mean\": matrix.mean(),\n",
    "        \"Std\": matrix.std(),\n",
    "        \"Median\": np.median(matrix),\n",
    "        \"L2 Norm\": np.linalg.norm(matrix).item(),\n",
    "        \"P99\": np.percentile(matrix, 99),\n",
    "        \"Diag Mean\": diag.mean(),\n",
    "        \"Avg Dist\": avg_dist,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_stats(stats: dict[str, float]) -> str:\n",
    "    \"\"\"Format statistics dictionary into a two-line string.\"\"\"\n",
    "    items = [\n",
    "        f\"{k}: {v:.4e}\" if k != \"Avg Dist\" else f\"{k}: {v:.2f}\"\n",
    "        for k, v in stats.items()\n",
    "    ]\n",
    "    line1 = \" | \".join(items[:5])\n",
    "    line2 = \" | \".join(items[5:])\n",
    "    return f\"{line1}<br>{line2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff326f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_mean_state: tuple[int, int, bool] = 0, 0, True\n",
    "\n",
    "\n",
    "def plot_mean(layer: int, head: int, mask_upper: bool) -> None:\n",
    "    \"\"\"Plot mean attention matrix for given layer and head.\"\"\"\n",
    "    global current_mean_state  # noqa: PLW0603\n",
    "    matrix = mean_attention_matrix[layer, head].numpy().copy()\n",
    "    stats = get_stats(matrix)\n",
    "    stats_str = format_stats(stats)\n",
    "\n",
    "    # Mask upper triangle\n",
    "    if mask_upper:\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "        matrix[mask] = np.nan\n",
    "\n",
    "    fig = px.imshow(\n",
    "        matrix,\n",
    "        color_continuous_scale=\"Viridis\",\n",
    "        title=f\"Attention Matrix Mean (Layer {layer}, Head {head})<br><sup>{stats_str}</sup>\",\n",
    "        labels={\"x\": \"Key Position\", \"y\": \"Query Position\", \"color\": \"Attention\"},\n",
    "    )\n",
    "    fig.update_layout(width=800, height=850, margin={\"t\": 120})\n",
    "    current_mean_state = layer, head, mask_upper\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "interact(\n",
    "    plot_mean,\n",
    "    layer=widgets.IntSlider(min=0, max=n_layers - 1, step=1, value=0),\n",
    "    head=widgets.IntSlider(min=0, max=n_heads - 1, step=1, value=0),\n",
    "    mask_upper=widgets.Checkbox(value=True, description=\"Mask Upper Triangle\"),\n",
    ")\n",
    "\n",
    "\n",
    "def export_mean_png(_b: widgets.Button) -> None:\n",
    "    \"\"\"Export the current mean attention matrix plot as a PNG file.\"\"\"\n",
    "    layer, head, mask_upper = current_mean_state\n",
    "    matrix = mean_attention_matrix[layer, head].numpy().copy()\n",
    "    if mask_upper:\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "        matrix[mask] = np.nan\n",
    "\n",
    "    plt.style.use([\"bright\", \"no-latex\"])\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.family\": \"sans-serif\",\n",
    "            \"font.sans-serif\": [\"DejaVu Sans\", \"Liberation Sans\", \"Arial\"],\n",
    "            \"mathtext.fontset\": \"dejavusans\",\n",
    "            \"font.size\": 25,\n",
    "            \"axes.labelsize\": 25,\n",
    "            \"xtick.labelsize\": 20,\n",
    "            \"ytick.labelsize\": 20,\n",
    "        },\n",
    "    )\n",
    "    file_name = (\n",
    "        f\"attention_heatmap_mean_{model_name}_{sequence_length}_{layer}_{head}.png\"\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(matrix, cmap=\"viridis\")\n",
    "    ax.set_xlabel(\"Key Position\")\n",
    "    ax.set_ylabel(\"Query Position\")\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax, label=\"Attention\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(result_dir / file_name, dpi=400, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Exported to {result_dir / file_name}\")\n",
    "\n",
    "\n",
    "button_png = widgets.Button(description=\"Export PNG\")\n",
    "button_png.on_click(export_mean_png)\n",
    "\n",
    "display(widgets.HBox([button_png]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_var_state: tuple[int, int, bool] = 0, 0, True\n",
    "\n",
    "\n",
    "def plot_variance(layer: int, head: int, mask_upper: bool) -> None:\n",
    "    \"\"\"Plot variance attention matrix for given layer and head.\"\"\"\n",
    "    global current_var_state  # noqa: PLW0603\n",
    "    matrix = var_attention_matrix[layer, head].numpy().copy()\n",
    "    stats = get_stats(matrix)\n",
    "    stats_str = format_stats(stats)\n",
    "\n",
    "    # Mask upper triangle\n",
    "    if mask_upper:\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "        matrix[mask] = np.nan\n",
    "\n",
    "    fig = px.imshow(\n",
    "        matrix,\n",
    "        color_continuous_scale=\"Viridis\",\n",
    "        title=f\"Attention Variance Matrix (Layer {layer}, Head {head})<br><sup>{stats_str}</sup>\",\n",
    "        labels={\"x\": \"Key Position\", \"y\": \"Query Position\", \"color\": \"Variance\"},\n",
    "    )\n",
    "    fig.update_layout(width=800, height=850, margin={\"t\": 120})\n",
    "    current_var_state = layer, head, mask_upper\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "interact(\n",
    "    plot_variance,\n",
    "    layer=widgets.IntSlider(min=0, max=n_layers - 1, step=1, value=0),\n",
    "    head=widgets.IntSlider(min=0, max=n_heads - 1, step=1, value=0),\n",
    "    mask_upper=widgets.Checkbox(value=True, description=\"Mask Upper Triangle\"),\n",
    ")\n",
    "\n",
    "\n",
    "def export_variance_png(_b: widgets.Button) -> None:\n",
    "    \"\"\"Export the current variance attention matrix plot as a PNG file.\"\"\"\n",
    "    layer, head, mask_upper = current_var_state\n",
    "    matrix = var_attention_matrix[layer, head].numpy().copy()\n",
    "    if mask_upper:\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "        matrix[mask] = np.nan\n",
    "\n",
    "    plt.style.use([\"bright\", \"no-latex\"])\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.family\": \"sans-serif\",\n",
    "            \"font.sans-serif\": [\"DejaVu Sans\", \"Liberation Sans\", \"Arial\"],\n",
    "            \"mathtext.fontset\": \"dejavusans\",\n",
    "            \"font.size\": 25,\n",
    "            \"axes.labelsize\": 25,\n",
    "            \"xtick.labelsize\": 20,\n",
    "            \"ytick.labelsize\": 20,\n",
    "        },\n",
    "    )\n",
    "    file_name = (\n",
    "        f\"attention_heatmap_variance_{model_name}_{sequence_length}_{layer}_{head}.png\"\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(matrix, cmap=\"viridis\")\n",
    "    ax.set_xlabel(\"Key Position\")\n",
    "    ax.set_ylabel(\"Query Position\")\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n",
    "    fig.colorbar(im, cax=cax, label=\"Variance\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(result_dir / file_name, dpi=400, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Exported to {result_dir / file_name}\")\n",
    "\n",
    "\n",
    "button_png = widgets.Button(description=\"Export PNG\")\n",
    "button_png.on_click(export_variance_png)\n",
    "\n",
    "display(widgets.HBox([button_png]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "position-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
